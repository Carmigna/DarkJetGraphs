{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability for Jetgraphs with Captum TracIn\n",
    "\n",
    "In this notebook we show how to apply the Captum TracIn explainability method to some basic Graph Neural Networks (GNNs), making use of the Jetgraph Dataset.\n",
    "\n",
    "Because Captum does not provide an integration with Pytorch Geometric yet, and I realized the code was a bit messy, I moved the overriding and technicalities inside the python package, so that one can just run the explainability method without worrying about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install captum\n",
    "!pip install --user annoy\n",
    "!pip install pytorch-lightning\n",
    "!pip install wandb\n",
    "!pip install git+https://github.com/alessiodevoto/jetgraphs.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some nice features for ipynb to improve plots visualization.\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(12345) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Captum and other packages\n",
    "# Move overriding to other package or same package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a model\n",
    "In order to apply Captum, we need a model trained on the dataset first. We must download the dataset *with the settings we want* and instantiate the model. \n",
    "\n",
    "For the dataset, we use the class defined in  `JetgraphDataset.py `. For the model, we can either pick any model from Pytorch Geometric or use one of the pre-defined ones in  `models.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Download dataset\n",
    "In the next lines we download the dataset with specific settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetgraphs.JetGraphDataset import JetGraphDatasetInMemory_v2\n",
    "from jetgraphs.transforms import BuildEdges\n",
    "from torch_geometric.transforms import Compose, LargestConnectedComponents\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Where data is to be downloaded and stored.\n",
    "datasets_root = \"./datasets\" \n",
    "# secret url to dataset.\n",
    "raw_data_url = \"\"\n",
    "\n",
    "# In the next lines we define how we build the dataset's edges and graph.\n",
    "\n",
    "# As discussed, we stick to 0.6x0.6 thresholds for the edges.\n",
    "edge_builder = BuildEdges(\n",
    "    directed=False, \n",
    "    self_loop_weight=1,\n",
    "    same_layer_threshold=0.6, \n",
    "    consecutive_layer_threshold=0.6,\n",
    "    distance_p=2)\n",
    "\n",
    "# We extract the main subgraph for each graph and one hot encode the layer.\n",
    "transforms = Compose([\n",
    "    LargestConnectedComponents(num_components=1),\n",
    "    # TODO one hot encode\n",
    "    ])\n",
    "\n",
    "# Finally download the dataset.\n",
    "jet_graph_dataset = JetGraphDatasetInMemory_v2(\n",
    "    root = datasets_root,       # directory where to download data \n",
    "    url = raw_data_url,         # url to raw data\n",
    "    subset = '100%',            # which subset of the intial 100k graph to consider, default is 100%\n",
    "    min_num_nodes = 2,          # only include graphs with at least 2 nodes \n",
    "    transform =  transforms,\n",
    "    pre_transform = edge_builder) # edge_builder should be passed as pre_transform to keep data on disk.\n",
    "\n",
    "# Create the dataloaders.\n",
    "train_idx, test_idx = train_test_split(range(len(jet_graph_dataset)), stratify=[m.y[0].item() for m in jet_graph_dataset], test_size=0.25)\n",
    "train_loader = DataLoader(jet_graph_dataset[train_idx], batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(jet_graph_dataset[test_idx], batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a model. \n",
    "# We take it from jetgraphs.models, but it can be any model for pytorch lightning.\n",
    "from jetgraphs.models import ShallowGCN\n",
    "\n",
    "model = ShallowGCN(hidden_channels=32, node_feat_size=jet_graph_dataset[0].x.shape[1])\n",
    "model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a model we need to provide a directory where to store checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as ptlight\n",
    "\n",
    "# Provide directory to store checkpoints and train (maybe move training inside package?)\n",
    "chkpt_dir = './checkpoints/'\n",
    "\n",
    "# We save checkpoints every 50 epochs \n",
    "checkpoint_callback = ptlight.callbacks.ModelCheckpoint(\n",
    "    dirpath=chkpt_dir,\n",
    "    filename='gnn-{epoch:02d}',\n",
    "    every_n_epochs=40,\n",
    "    save_top_k=-1)\n",
    "\n",
    "# Define trainer.\n",
    "trainer = ptlight.Trainer(\n",
    "    default_root_dir=chkpt_dir, \n",
    "    max_epochs=200, \n",
    "    callbacks=[checkpoint_callback])\n",
    "\n",
    "# Train model.\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Captum TracIn\n",
    "Once we have the model checkpoints saved, we can run Captum TracIn. \n",
    "Please recall that Captum TracIn has two implementations: the fast one, that works on all models, and the complete one, that does not work on ArmaConv and GatConv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import captum from the jetgraphs repo.\n",
    "from jetgraphs.explainability import TracInCPFastGNN, TracInCPGNN, checkpoints_load_func\n",
    "import os.path as osp\n",
    "\n",
    "# We first load the model with the last checkpoint so that the predictions we make in the next cell will be for the trained model.\n",
    "correct_dataset_final_checkpoint = osp.join(chkpt_dir, 'gnn-epoch=149.ckpt')\n",
    "checkpoints_load_func(model, correct_dataset_final_checkpoint)\n",
    "\n",
    "# Dataloader for Captum.\n",
    "test_influence_indices = test_idx[:4]  # Just consider the first 4 examples in validation dataset\n",
    "test_influence_loader = DataLoader(jet_graph_dataset[test_influence_indices], batch_size=len(test_influence_indices), shuffle=False)\n",
    "test_examples_batch = next(iter(test_influence_loader))\n",
    "\n",
    "influence_src_dataloader = DataLoader(jet_graph_dataset[train_idx], batch_size=64, shuffle=False)\n",
    "test_examples_predicted_probs = torch.sigmoid(model(test_examples_batch)) \n",
    "test_examples_predicted_labels = (test_examples_predicted_probs > 0.5).float()\n",
    "test_examples_true_labels = test_examples_batch.y.unsqueeze(1)\n",
    "\n",
    "# Run Captum\n",
    "tracin_cp_fast_gnn = TracInCPFastGNN(\n",
    "    model=model,\n",
    "    final_fc_layer=list(model.children())[-1],\n",
    "    influence_src_dataset=influence_src_dataloader,\n",
    "    checkpoints=chkpt_dir,\n",
    "    checkpoints_load_func=checkpoints_load_func,\n",
    "    loss_fn=torch.nn.functional.binary_cross_entropy_with_logits,\n",
    "    batch_size=2048,\n",
    "    vectorize=False,\n",
    ")\n",
    "\n",
    "import datetime\n",
    "\n",
    "k = 4\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "proponents_indices_fast, proponents_influence_scores_fast = tracin_cp_fast_gnn.influence(\n",
    "    inputs = test_examples_batch, \n",
    "    targets = test_examples_true_labels, \n",
    "    k=k, \n",
    "    proponents=True, \n",
    "    unpack_inputs=False\n",
    ")\n",
    "\n",
    "\n",
    "opponents_indices_fast, opponents_influence_scores_fast = tracin_cp_fast_gnn.influence(\n",
    "    inputs = test_examples_batch, \n",
    "    targets = test_examples_true_labels, \n",
    "    k=k, \n",
    "    proponents=False, \n",
    "    unpack_inputs=False\n",
    ")\n",
    "\n",
    "total_minutes = (datetime.datetime.now() - start_time).total_seconds() / 60.0\n",
    "\n",
    "print(\n",
    "    \"Computed proponents / opponents over a dataset of %d examples in %.2f minutes\"\n",
    "    % (len(influence_src_dataloader)*influence_src_dataloader.batch_size, total_minutes)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the results in terms of proponents and opponents, leveraging the  `plot_jet_graph ` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "from jetgraphs.explainability import display_proponents_and_opponents\n",
    "\n",
    "# Reconstruct the correct dataset from dataloder\n",
    "src_dataset = []\n",
    "for x in influence_src_dataloader:\n",
    "  src_dataset.extend(x.to_data_list())\n",
    "\n",
    "display_proponents_and_opponents(\n",
    "  test_examples_batch.to_data_list(),\n",
    "  src_dataset,\n",
    "  test_examples_true_labels,\n",
    "  test_examples_predicted_labels,\n",
    "  test_examples_predicted_probs,\n",
    "  proponents_indices_fast,\n",
    "  opponents_indices_fast\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gnns')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbf4f76e219583883e8ae7ee521157ca35bbe5b3f94f071c5a94dd88b5bedd86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
